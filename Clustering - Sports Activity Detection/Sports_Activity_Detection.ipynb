{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygqMuuXcKsx3"
      },
      "source": [
        "# <font color='black' size='7px'> ***Global Variables***</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rp_bSNS_LfpX"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQcuv3KLCTx"
      },
      "source": [
        "# <font color='black' size='7px'> ***Kaggle Data Configuration***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXI8HMGKFxxi"
      },
      "source": [
        "## Hold content folder by kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EWeTdMVD8Zrv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqXEzv8NMK8L"
      },
      "source": [
        "##  *Dataset Landing*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp4Hmdz78fUm"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d obirgul/daily-and-sports-activities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "19Dum26bcRkT"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# unzip all files\n",
        "with zipfile.ZipFile('daily-and-sports-activities.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSJhECMNKOQX"
      },
      "source": [
        "# <font color='black' size='7px'> ***Dataset Loader***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQAB-SBLL5QG"
      },
      "source": [
        "## *Helper Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O7pJTN5RL-g-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def formulatedImage(pixels):\n",
        "  return np.array(pixels).reshape(-1)\n",
        "\n",
        "def get_truth_value_for_segment(activity_folder, subject_folder, segment_file):\n",
        "  return int(activity_folder[1:])\n",
        "\n",
        "def plot_image(image_array, resized):\n",
        "  '''\n",
        "  parameter: 1d-array, bool value\n",
        "  '''\n",
        "  if resized:\n",
        "    plt.imshow(image_array.reshape(112 // 2, 92 // 2), cmap='gray')  # Use 'cmap=None' if the image is colored\n",
        "  else:\n",
        "    plt.imshow(image_array.reshape(112, 92), cmap='gray')\n",
        "  plt.axis('off')  # Turn off axis\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzG7HGpEMCiP"
      },
      "source": [
        "## *Data Loader*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vCMjCNoCMO7P"
      },
      "outputs": [],
      "source": [
        "import imageio as img\n",
        "import pandas as pd\n",
        "\n",
        "def basic_load_for_dataset(absolute_directory_path):\n",
        "  '''\n",
        "  return naive-data: shape = (19, 8, 60, 125, 45) and naive-truth: shape = (19, 8, 60)\n",
        "  '''\n",
        "\n",
        "  naive_data = []\n",
        "  naive_truth = []\n",
        "  for activity_folder in sorted(os.listdir(absolute_directory_path)):\n",
        "      activity_path = os.path.join(absolute_directory_path, activity_folder)\n",
        "      activity_data = []\n",
        "      activity_truth = []\n",
        "      for subject_folder in sorted(os.listdir(activity_path)):\n",
        "          subject_path = os.path.join(activity_path, subject_folder)\n",
        "          segment_data = []\n",
        "          segment_truth = []\n",
        "          for segment_file in sorted(os.listdir(subject_path)):\n",
        "              segment_path = os.path.join(subject_path, segment_file)\n",
        "              df = pd.read_csv(segment_path, header=None)\n",
        "              segment_data.append(df.values.tolist())\n",
        "              segment_truth.append(get_truth_value_for_segment(activity_folder, subject_folder, segment_file))\n",
        "          activity_data.append(segment_data)\n",
        "          activity_truth.append(segment_truth)\n",
        "      naive_data.append(activity_data)\n",
        "      naive_truth.append(activity_truth)\n",
        "  return np.array(naive_data), np.array(naive_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ktqUbUFZwL"
      },
      "source": [
        "## *Data splitter*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AVANpcX2Felm"
      },
      "outputs": [],
      "source": [
        "def split_data(naive_data, naive_truth, ratio):\n",
        "  '''\n",
        "  parameter: array: shape = (19, 8, 60, 125, 45) & array: shape = (19, 8, 60) & ratio of type float.\n",
        "  return:\n",
        "    4 arrays of shape =\n",
        "      (19, 8, 60 * ratio, 125, 45) &\n",
        "      (19, 8, 60 * ratio) &\n",
        "      (19, 8, 60 * (1 - ratio), 125, 45) &\n",
        "      (19, 8, 60 * (1 - ratio)\n",
        "  '''\n",
        "  dimension_to_split_at = 2\n",
        "  splitted_dimension_size = naive_data.shape[dimension_to_split_at]\n",
        "  split_size = int(ratio * splitted_dimension_size)\n",
        "\n",
        "  training_data = naive_data[:, :, :split_size, :, :]\n",
        "  training_truth = naive_truth[:, :, :split_size]\n",
        "  testing_data = naive_data[:, :, split_size:, :, :]\n",
        "  testing_truth = naive_truth[:, :, split_size:]\n",
        "\n",
        "  return training_data, training_truth, testing_data, testing_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ujoh7TLzXvZ"
      },
      "source": [
        "## *Make dataset in memory*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NHzmV-Mjzedh"
      },
      "outputs": [],
      "source": [
        "naive_data, naive_truth = basic_load_for_dataset(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkIBxUexaElo"
      },
      "source": [
        "# <font color='black' size='7px'> ***Preprocessing***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xmdm0n7M9Cq"
      },
      "source": [
        "## *Reformulate dataset using mean*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PRxg7P39NXma"
      },
      "outputs": [],
      "source": [
        "def dataset_after_mean_reformulation(train_data, train_truth):\n",
        "  '''\n",
        "  parameter: array of shape = (19, 8, number_of_segments, 125, 45)\n",
        "  return:\n",
        "    array of shape (19 * 8 * number_of_segments, 45) which is the data matrix needed for each model..\n",
        "    array of shape (19 * 8 * number_of_segments) which is the label vector..\n",
        "  '''\n",
        "\n",
        "  # get number_of_segments\n",
        "  number_of_segments = train_data.shape[2]\n",
        "\n",
        "  # compress first 3 dimensions because we don't need them.\n",
        "  collapsed_arr = train_data.reshape((19 * 8 * number_of_segments, 125, 45))\n",
        "  collapsed_truth = train_truth.reshape((19 * 8 * number_of_segments))\n",
        "\n",
        "  # mean_of_each_set will be a numpy array of shape (19 * 8 * number_of_segments, 45)\n",
        "  mean_of_each_set = np.mean(collapsed_arr, axis=1)\n",
        "\n",
        "  return mean_of_each_set, collapsed_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Ha7UHXNJkZ"
      },
      "source": [
        "## *Reformulate dataset using dimensionality reduction*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wAnwINCXNsPx"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def dataset_after_reduction_reformulation(train_data, train_truth):\n",
        "  '''\n",
        "  parameter: array of shape = (19, 8, number_of_segments, 125, 45)\n",
        "  return:\n",
        "    pca model and\n",
        "    array of shape (19 * 8 * number_of_segments, number_components_of_pca(125 * 45)) which is the data matrix needed for each model..\n",
        "    array of shape (19 * 8 * number_of_segments) which is the label vector..\n",
        "  '''\n",
        "\n",
        "  # get number_of_segments\n",
        "  number_of_segments = train_data.shape[2]\n",
        "\n",
        "  # reshape the naive_data to be on standard shape n * d...\n",
        "  collapsed_arr = train_data.reshape((19 * 8 * number_of_segments, 125 * 45))\n",
        "  collapsed_truth = train_truth.reshape((19 * 8 * number_of_segments))\n",
        "\n",
        "  # create pca object\n",
        "  desired_number_of_components = 45\n",
        "  pca = PCA(n_components=desired_number_of_components)\n",
        "\n",
        "  # Fit PCA to your data\n",
        "  pca.fit(collapsed_arr)\n",
        "\n",
        "  # Transform your data to the new feature space\n",
        "  transformed_data = pca.transform(collapsed_arr)\n",
        "\n",
        "  return pca, transformed_data, collapsed_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BchZOHHQyayB"
      },
      "source": [
        "# <font color='black' size='7px'> ***Start point for algorithms***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWZx8zD50K9T"
      },
      "source": [
        "## *Mean Dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7V2YBWbUyiqZ"
      },
      "outputs": [],
      "source": [
        "def get_mean_dataset(naive_data, naive_truth, ratio):\n",
        "  '''\n",
        "  parameter: basic naive -in memory- dataset(19, 8, 60, 125, 45), basic naive -in memory- ground_truth(19, 8, 60), ratio between 0 and 1\n",
        "  return:\n",
        "    train_dataset (19 * 8 * number_of_segments_training, 45) &\n",
        "    train_truth (19 * 8 * number_of_segments_training)\n",
        "    test_dataset (19, 8, number_of_segments_testing, 125, 45)\n",
        "    test_truth (19, 8, number_of_segments_testing)\n",
        "  '''\n",
        "  train_dataset, train_truth, test_dataset, test_truth = split_data(naive_data, naive_truth, ratio)\n",
        "  train_mean_dataset, train_mean_truth = dataset_after_mean_reformulation(train_dataset, train_truth)\n",
        "  return train_mean_dataset, train_mean_truth, test_dataset, test_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUJUn8oT0Tn6"
      },
      "source": [
        "## *Reduced Dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S8wzg0xY0X6b"
      },
      "outputs": [],
      "source": [
        "def get_reduced_dataset(naive_data, naive_truth, ratio):\n",
        "  '''\n",
        "  parameter: basic naive -in memory- dataset(19, 8, 60, 125, 45), ratio between 0 and 1\n",
        "  return:\n",
        "    pca model,\n",
        "    training data of shape (19 * 8 * number_of_segments_training, number_components_of_pca(125 * 45)) which is the data matrix needed for each model,\n",
        "    training ground truth of shape(19 * 8 * number_of_segments_training)\n",
        "    test_dataset (19, 8, number_of_segments_testing, 125, 45)\n",
        "    test_truth (19, 8, number_of_segments_testing)\n",
        "  '''\n",
        "  train_dataset, train_truth, test_dataset, test_truth = split_data(naive_data, naive_truth, ratio)\n",
        "  pca, train_reduced_dataset, train_reduced_truth = dataset_after_reduction_reformulation(train_dataset, train_truth)\n",
        "  return pca, train_reduced_dataset, train_reduced_truth, test_dataset, test_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-7O8fBdt1Hh"
      },
      "source": [
        "# <font color='black' size='7px'> ***Evaluation***</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4Foqp-RdfXla"
      },
      "outputs": [],
      "source": [
        "def count(i, j, predictions, truth):\n",
        "  count = 0\n",
        "  for idx in range(len(predictions)):\n",
        "      if predictions[idx] == i and truth[idx] == j:\n",
        "          count += 1\n",
        "  return count\n",
        "\n",
        "def map(predictions, truth):\n",
        "  truth_classes_num = np.unique(np.array(truth)).shape[0] + 1\n",
        "  predicted_classes_num = np.unique(np.array(predictions)).shape[0] + 1\n",
        "\n",
        "  matrix = np.zeros((predicted_classes_num, truth_classes_num))\n",
        "\n",
        "  for i in range(predicted_classes_num):\n",
        "    for j in range(truth_classes_num):\n",
        "      matrix[i][j] = count(i, j, predictions, truth)\n",
        "\n",
        "  return matrix\n",
        "\n",
        "def total_precision(matrix):\n",
        "    precision_classes = np.zeros(len(matrix))\n",
        "    total = 0\n",
        "    total_occurences = np.sum(matrix)\n",
        "\n",
        "    for i in range(len(precision_classes)):\n",
        "      max_occurence = np.max(matrix[i])\n",
        "      class_occurence = np.sum(matrix[i])\n",
        "      if np.sum(matrix[i]):\n",
        "        precision_classes[i] = max_occurence / np.sum(matrix[i])\n",
        "      if total_occurences != 0:\n",
        "        total += max_occurence / total_occurences\n",
        "\n",
        "    return total, precision_classes\n",
        "\n",
        "def total_recall(matrix):\n",
        "    recall_classes = np.zeros(len(matrix))\n",
        "    total = 0\n",
        "\n",
        "    for i in range(len(recall_classes)):\n",
        "      max_ind = np.argmax(matrix[i])\n",
        "      val = np.sum(matrix[:, max_ind])\n",
        "      if val != 0:\n",
        "        recall_classes[i] = matrix[i][max_ind] / val\n",
        "      total += recall_classes[i] / len(matrix)\n",
        "\n",
        "    return total, recall_classes\n",
        "\n",
        "def f_measure(precision, recall):\n",
        "    total = 0\n",
        "    for i in range(len(precision)):\n",
        "      if precision[i] + recall[i] > 0:\n",
        "        total += 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
        "\n",
        "    return total / (len(precision) - 1)\n",
        "\n",
        "def entropy(matrix):\n",
        "    entropy_classes = np.zeros(len(matrix))\n",
        "    total = 0\n",
        "    total_occurences = np.sum(matrix)\n",
        "\n",
        "    for i in range(len(entropy_classes)):\n",
        "      cluster_sum = np.sum(matrix[i])\n",
        "      for j in range(len(matrix[i])):\n",
        "        if cluster_sum > 0 and matrix[i][j] / cluster_sum > 0:\n",
        "         entropy_classes[i] -= matrix[i][j] / cluster_sum * np.log2(matrix[i][j] / cluster_sum)\n",
        "      if total_occurences != 0:\n",
        "        total += entropy_classes[i] * (cluster_sum / total_occurences)\n",
        "\n",
        "    return total, entropy_classes\n",
        "\n",
        "\n",
        "def evaluate(predictions, truth):\n",
        "  contingency_matrix = map(predictions, truth)\n",
        "  total_pre, precision_classes = total_precision(contingency_matrix)\n",
        "  total_rec, recall_classes = total_recall(contingency_matrix)\n",
        "  f_score = f_measure(precision_classes, recall_classes)\n",
        "  entropy_value, entropy_classes = entropy(contingency_matrix)\n",
        "  print(\"\\tPrecision : \", total_pre)\n",
        "  print(\"\\tRecall : \", total_rec)\n",
        "  print(\"\\tF-score : \", f_score)\n",
        "  print(\"\\tEntropy : \", entropy_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1RcBaSYs7JA"
      },
      "source": [
        "# <font color='black' size='7px'> ***K-Means***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phX3JNTgtBKE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_DxAbLk2s9EM"
      },
      "outputs": [],
      "source": [
        "def genrate_randome_centroids(dataset, clusters_num):\n",
        "  return dataset[np.random.randint(0, dataset.shape[0], clusters_num)]\n",
        "\n",
        "def predict_cluster_for_point(point, centroids):\n",
        "  distances = []\n",
        "  for centroid in centroids:\n",
        "    distances.append(np.linalg.norm(point - centroid))\n",
        "  return np.argmin(distances)\n",
        "\n",
        "def small_difference(centroids, old_centroids, err):\n",
        "  for i in range(centroids.shape[0]):\n",
        "    if np.linalg.norm(centroids[i] - old_centroids[i]) > err:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def k_means_training(dataset, clusters_num, err):\n",
        "  centroids = genrate_randome_centroids(dataset, clusters_num)\n",
        "  old_centroids = np.zeros((clusters_num, dataset.shape[1]))\n",
        "  while not small_difference(centroids, old_centroids, err):\n",
        "    old_centroids = np.array(centroids)\n",
        "    clusters = [[] for _ in range(clusters_num)]\n",
        "    for point in dataset:\n",
        "      assigned_cluster = predict_cluster_for_point(point, centroids)\n",
        "      clusters[assigned_cluster].append(point)\n",
        "    for i in range(clusters_num):\n",
        "      if len(clusters[i]) != 0:\n",
        "        centroids[i] = np.mean(clusters[i])\n",
        "  return centroids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysPHfsjDtGZ1"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eqED9YaItDX4"
      },
      "outputs": [],
      "source": [
        "def k_means_testing(testing_dataset, centroids):\n",
        "  testing_samples_size = testing_dataset.shape[0]\n",
        "  predictions = np.zeros((testing_samples_size))\n",
        "  for i in range(testing_samples_size):\n",
        "    predictions[i] = predict_cluster_for_point(testing_dataset[i], centroids)\n",
        "  predictions = np.array(predictions, dtype=int)\n",
        "  return predictions;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UDCATJYtvtr"
      },
      "source": [
        "## K-Means Caller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqKodE4Ntyys"
      },
      "source": [
        "### Train and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean reformulation"
      ],
      "metadata": {
        "id": "J3n1Ff3xt5Pu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I0lTfh1cttdP"
      },
      "outputs": [],
      "source": [
        "training_with_mean_reformulation, train_truth, testing_with_mean_reformulation, test_truth = get_mean_dataset(naive_data,naive_truth, 0.8)\n",
        "testing_with_mean_reformulation, test_truth = dataset_after_mean_reformulation(testing_with_mean_reformulation, test_truth)\n",
        "clusters_number = [8, 13, 19, 28,38]\n",
        "# clusters_number = [19]\n",
        "predictions = []\n",
        "predictions_training = []\n",
        "for k in clusters_number:\n",
        "  centroids = k_means_training(training_with_mean_reformulation, k, 0.000001)\n",
        "  predictions.append(k_means_testing(training_with_mean_reformulation, centroids))\n",
        "  predictions_training.append(k_means_testing(testing_with_mean_reformulation, centroids))\n",
        "predictions = np.array(predictions)\n",
        "predictions_training = np.array(predictions_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA reformulation"
      ],
      "metadata": {
        "id": "vltDG0LFt9Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca, training_with_pca_reformulation, train_truth, testing_with_pca_reformulation, test_truth = get_reduced_dataset(naive_data,naive_truth, 0.8)\n",
        "collapsed_arr = testing_with_pca_reformulation.reshape((19 * 8 * 12, 125 * 45))\n",
        "test_truth = test_truth.reshape((19*8*12, 1))\n",
        "testing_with_pca_reformulation = pca.transform(collapsed_arr)\n",
        "clusters_number = [8, 13, 19, 28,38]\n",
        "predictions_pca = []\n",
        "predictions_testing_pca = []\n",
        "for k in clusters_number:\n",
        "  centroids = k_means_training(training_with_pca_reformulation, k, 0.000001)\n",
        "  predictions_pca.append(k_means_testing(training_with_pca_reformulation, centroids))\n",
        "  predictions_testing_pca.append(k_means_testing(testing_with_pca_reformulation, centroids))\n",
        "predictions_pca = np.array(predictions_pca)\n",
        "testing_with_pca_reformulation = np.array(testing_with_pca_reformulation)"
      ],
      "metadata": {
        "id": "8Pmo9xVzsXHR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-means Evaluation"
      ],
      "metadata": {
        "id": "iTxdOkCcuQG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means mean refoemulation evaluation"
      ],
      "metadata": {
        "id": "6217bNXFuyth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training set evaluation"
      ],
      "metadata": {
        "id": "R4WCaHR6vv_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(predictions.shape[0]):\n",
        "  print(f\"At K = {clusters_number[i]}\")\n",
        "  evaluate(predictions[i], train_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYf1uGZ3iyH_",
        "outputId": "6889350a-98a5-4d94-e41a-cc6f4593f5a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At K = 8\n",
            "\tPrecision :  0.22957785087719296\n",
            "\tRecall :  0.48466435185185186\n",
            "\tF-score :  0.3539006059195943\n",
            "\tEntropy :  3.0771250420461045\n",
            "At K = 13\n",
            "\tPrecision :  0.25137061403508776\n",
            "\tRecall :  0.34114583333333337\n",
            "\tF-score :  0.29894694390478577\n",
            "\tEntropy :  2.940991443824103\n",
            "At K = 19\n",
            "\tPrecision :  0.27782346491228066\n",
            "\tRecall :  0.26393229166666665\n",
            "\tF-score :  0.2760021917441681\n",
            "\tEntropy :  2.8420742950837687\n",
            "At K = 28\n",
            "\tPrecision :  0.28728070175438597\n",
            "\tRecall :  0.1882183908045977\n",
            "\tF-score :  0.23182960451797943\n",
            "\tEntropy :  2.7638914313286524\n",
            "At K = 38\n",
            "\tPrecision :  0.3022203947368421\n",
            "\tRecall :  0.1472355769230769\n",
            "\tF-score :  0.19535078478107218\n",
            "\tEntropy :  2.705486681561164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing evaluation"
      ],
      "metadata": {
        "id": "KmXhW7qryS7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(predictions_training.shape[0]):\n",
        "  print(f\"At K = {clusters_number[i]}\")\n",
        "  evaluate(predictions_training[i], test_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhCxwpcCye95",
        "outputId": "96571740-6e42-46a6-f64f-fcb6de9019db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At K = 8\n",
            "\tPrecision :  0.21655701754385964\n",
            "\tRecall :  0.45717592592592593\n",
            "\tF-score :  0.34029599410747347\n",
            "\tEntropy :  3.1048410108876943\n",
            "At K = 13\n",
            "\tPrecision :  0.25109649122807015\n",
            "\tRecall :  0.34077380952380953\n",
            "\tF-score :  0.29781880917778103\n",
            "\tEntropy :  2.9196753542935396\n",
            "At K = 19\n",
            "\tPrecision :  0.26206140350877194\n",
            "\tRecall :  0.24895833333333334\n",
            "\tF-score :  0.2512336040873487\n",
            "\tEntropy :  2.7965193573303355\n",
            "At K = 28\n",
            "\tPrecision :  0.29769736842105265\n",
            "\tRecall :  0.19504310344827586\n",
            "\tF-score :  0.23213003856031675\n",
            "\tEntropy :  2.6282322115984798\n",
            "At K = 38\n",
            "\tPrecision :  0.319078947368421\n",
            "\tRecall :  0.15544871794871792\n",
            "\tF-score :  0.20614760522246867\n",
            "\tEntropy :  2.536616140589109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means PCA reformulation evaluarion"
      ],
      "metadata": {
        "id": "HrLtEE36u4Tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train set evalauation"
      ],
      "metadata": {
        "id": "FHyFWGoZzLUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(predictions_pca.shape[0]):\n",
        "  print(f\"At K = {clusters_number[i]}\")\n",
        "  evaluate(predictions_pca[i], train_truth)"
      ],
      "metadata": {
        "id": "YnXQCCuatUFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1daef9cb-0b00-4bd3-ea2c-cea8c27ddaff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At K = 8\n",
            "\tPrecision :  0.1758497807017544\n",
            "\tRecall :  0.37123842592592593\n",
            "\tF-score :  0.2837670133381044\n",
            "\tEntropy :  3.490870859187373\n",
            "At K = 13\n",
            "\tPrecision :  0.20010964912280702\n",
            "\tRecall :  0.27157738095238093\n",
            "\tF-score :  0.23988249507683274\n",
            "\tEntropy :  3.3827448746052906\n",
            "At K = 19\n",
            "\tPrecision :  0.2109375\n",
            "\tRecall :  0.20039062500000002\n",
            "\tF-score :  0.21562783772901584\n",
            "\tEntropy :  3.3402701339837013\n",
            "At K = 28\n",
            "\tPrecision :  0.2345120614035088\n",
            "\tRecall :  0.15364583333333334\n",
            "\tF-score :  0.18064035058516942\n",
            "\tEntropy :  3.281675534808273\n",
            "At K = 38\n",
            "\tPrecision :  0.24972587719298242\n",
            "\tRecall :  0.12166132478632477\n",
            "\tF-score :  0.1614380945600881\n",
            "\tEntropy :  3.2377499740341795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test evaluation"
      ],
      "metadata": {
        "id": "Zx9eynx2kTIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(predictions_pca.shape[0]):\n",
        "  print(f\"At K = {clusters_number[i]}\")\n",
        "  evaluate(predictions_testing_pca[i], test_truth)"
      ],
      "metadata": {
        "id": "VwraxSCVdXnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8795051-d4b5-42b5-ccc7-847103e5547a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At K = 8\n",
            "\tPrecision :  0.1836622807017544\n",
            "\tRecall :  0.3877314814814815\n",
            "\tF-score :  0.29413956802253954\n",
            "\tEntropy :  3.429138283319883\n",
            "At K = 13\n",
            "\tPrecision :  0.2028508771929824\n",
            "\tRecall :  0.275297619047619\n",
            "\tF-score :  0.24179666721830845\n",
            "\tEntropy :  3.305044356638809\n",
            "At K = 19\n",
            "\tPrecision :  0.21217105263157895\n",
            "\tRecall :  0.20156249999999998\n",
            "\tF-score :  0.21049259230909514\n",
            "\tEntropy :  3.2358689539971905\n",
            "At K = 28\n",
            "\tPrecision :  0.22532894736842102\n",
            "\tRecall :  0.14762931034482757\n",
            "\tF-score :  0.17377712862496061\n",
            "\tEntropy :  3.163233120289483\n",
            "At K = 38\n",
            "\tPrecision :  0.25712719298245607\n",
            "\tRecall :  0.12526709401709404\n",
            "\tF-score :  0.16613142199496866\n",
            "\tEntropy :  3.052770250624569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo6dFoaMWkhy"
      },
      "source": [
        "# <font color='black' size='7px'> ***Normalized Cut***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVrqNWdVWkiA"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P_2Dfv9nWkiB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "def normalize_eigenvectors(eigenvectors):\n",
        "  '''\n",
        "  parameter: The eigenvectors of the noramlized laplacian matrix\n",
        "  return: The noramlized eigen vectors\n",
        "  '''\n",
        "  normalized = []\n",
        "  for i in range(len(eigenvectors)):\n",
        "    ui = eigenvectors[i]\n",
        "    product = 1;\n",
        "    for x in ui:\n",
        "      product = product + x * x\n",
        "    z = 1 / np.sqrt(product)\n",
        "    normalized.append(z * ui)\n",
        "\n",
        "  return normalized\n",
        "\n",
        "def compute_similarity_matrix(data, gamma):\n",
        "  '''\n",
        "  parameter:\n",
        "    - The data matrix containing all the data points\n",
        "    - The gamma value that will be used for rbf function\n",
        "  return:\n",
        "    - The similarity matrix resulted from applying the rbf kernel function\n",
        "      with specified gamma value on all pairwise points\n",
        "  '''\n",
        "  n = data.shape[0]\n",
        "  similarity_matrix = np.zeros((n, n))\n",
        "  for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "          distance = np.linalg.norm(data[i] - data[j])\n",
        "          similarity_matrix[i, j] = similarity_matrix[j, i] = np.exp(-gamma * distance ** 2)\n",
        "  return similarity_matrix\n",
        "\n",
        "def k_way_normalized_cut(data, k, gamma):\n",
        "  '''\n",
        "  parameter:\n",
        "    - The data matrix containing all the data points\n",
        "    - The gamma value that will be used for generation of similarity matrix\n",
        "    - The number of clusters that will be generated\n",
        "  return:\n",
        "    - The Labels of each data point in the data matrix\n",
        "  '''\n",
        "  # Compute similarity matrix using RBF kernel\n",
        "  similarity_matrix = compute_similarity_matrix(data, gamma)\n",
        "\n",
        "  # Compute the degree matrix\n",
        "  degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n",
        "\n",
        "  # Compute the Laplacian matrix\n",
        "  laplacian_matrix = degree_matrix - similarity_matrix\n",
        "\n",
        "  # Compute the normalized Laplacian matrix (B matrix)\n",
        "  for i in range(len(laplacian_matrix)):\n",
        "    laplacian_matrix[i][i] /= degree_matrix[i][i]\n",
        "  normalized_laplacian_matrix = laplacian_matrix\n",
        "\n",
        "  # Compute the first k normalized eigenvectors\n",
        "  _, eigenvectors = np.linalg.eig(normalized_laplacian_matrix)\n",
        "  eigenvectors = eigenvectors[:, :k]\n",
        "  eigenvectors = np.real(eigenvectors)\n",
        "\n",
        "  # Compute the normalized eigenvectors\n",
        "  normalized_eigenvectors = normalize_eigenvectors(eigenvectors)\n",
        "\n",
        "  # Perform Kmeans on the normalized eigenvectors and get the labels of each point\n",
        "  labels = KMeans(n_clusters=k, random_state=0).fit(normalized_eigenvectors).labels_\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC8p6K4dVC4m"
      },
      "source": [
        "## Evaluation on mean dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the train of mean dataset in the model"
      ],
      "metadata": {
        "id": "QFYB9ynGWCYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the mean dataset\n",
        "train_mean_dataset, train_mean_truth, test_mean_dataset, test_mean_truth = get_mean_dataset(naive_data, naive_truth, 0.8)\n",
        "\n",
        "# Reformulate the test data\n",
        "test_mean_dataset, test_mean_truth = dataset_after_mean_reformulation(test_mean_dataset, test_mean_truth)\n",
        "\n",
        "# perfrom normalized cut on test mean dataset\n",
        "test_predictions = k_way_normalized_cut(test_mean_dataset, k=19, gamma=0.00001) + 1"
      ],
      "metadata": {
        "id": "0R8nN0eFWAg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the clusters using different measures"
      ],
      "metadata": {
        "id": "546utitbWIB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation on mean dataset: \")\n",
        "evaluate(test_predictions, test_mean_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynoP6CYCWB8n",
        "outputId": "99a341e9-57c4-4f34-b70b-630e858c1dc9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on mean dataset: \n",
            "\tPrecision :  0.4281798245614035\n",
            "\tRecall :  0.40677083333333325\n",
            "\tF-score :  0.42622502940551155\n",
            "\tEntropy :  2.02945440362094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULOT0SAfVxNz"
      },
      "source": [
        "## Evaluation on PCA dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the train of PCA dataset in the model"
      ],
      "metadata": {
        "id": "a_cpwXI1WTEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the PCA dataset\n",
        "pca, train_pca_dataset, train_pca_truth, test_pca_dataset, test_pca_truth = get_reduced_dataset(naive_data, naive_truth, 0.8)\n",
        "\n",
        "# Reformulate the PCA dataset\n",
        "collapsed_arr = test_pca_dataset.reshape((19 * 8 * 12, 125 * 45))\n",
        "collapsed_truth = test_pca_truth.reshape((19 * 8 * 12))\n",
        "\n",
        "# Reduce the dimensinos of test pca\n",
        "test_pca_dataset = pca.transform(collapsed_arr)\n",
        "\n",
        "# perfrom normalized cut on test PCA dataset\n",
        "test_pca_predictions = k_way_normalized_cut(test_pca_dataset, k=19, gamma=0.00001) + 1"
      ],
      "metadata": {
        "id": "PeiaTyLoWTE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the clusters using different measures"
      ],
      "metadata": {
        "id": "lsn03AajWTE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation on PCA dataset\")\n",
        "evaluate(test_pca_predictions, collapsed_truth)"
      ],
      "metadata": {
        "id": "iP1omxxtWTFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49970a4-0b6b-4c44-ad59-229eb576f75f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on PCA dataset\n",
            "\tPrecision :  0.43421052631578944\n",
            "\tRecall :  0.4124999999999999\n",
            "\tF-score :  0.4314414763549648\n",
            "\tEntropy :  2.086570185952532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQzBskuQGIvb"
      },
      "source": [
        "# <font color='black' size='7px'> ***DBSCAN***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global lists"
      ],
      "metadata": {
        "id": "hY39zqWjFn0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_counter = 0"
      ],
      "metadata": {
        "id": "40dM5HRpFikb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls0_ndiVGIvy"
      },
      "source": [
        "## Helpful Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sDXTJo9OGIvy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def get_core_indices(D, n, ε, min_points):\n",
        "  neighbors_model = NearestNeighbors(radius=ε, metric=\"euclidean\")\n",
        "  neighbors_model.fit(D)\n",
        "  neighborhoods = neighbors_model.radius_neighbors(D, return_distance=False)\n",
        "  n_neighbors = np.array([len(neighbors) for neighbors in neighborhoods])\n",
        "  core_samples = np.asarray(n_neighbors >= min_points, dtype=np.uint8)\n",
        "  core_sample_indices_ = np.where(core_samples)[0]\n",
        "  return core_sample_indices_\n",
        "\n",
        "def connect_to_adj_cores(\n",
        "    D, n, ε,\n",
        "    basic_core_node_index, cluster_to_assign,\n",
        "    core_indices_set, cluster_id\n",
        "  ):\n",
        "\n",
        "  stack = [basic_core_node_index]\n",
        "  while stack:\n",
        "    current_core_node_index = stack.pop()\n",
        "    for instance_index in range(n):\n",
        "      if ((np.linalg.norm(D[instance_index] - D[current_core_node_index])) > ε) or (cluster_id[instance_index] != -1): continue\n",
        "      cluster_id[instance_index] = cluster_to_assign\n",
        "      if instance_index in core_indices_set: stack.append(instance_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm"
      ],
      "metadata": {
        "id": "oN3dV7hJ2n9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DBSCAN_algorithm(D, ε, min_points):\n",
        "  global global_counter\n",
        "\n",
        "  # initialization attributes:\n",
        "  n = D.shape[0]\n",
        "  d = D.shape[1]\n",
        "  cluster_id = [-1] * n\n",
        "\n",
        "  # get core points:\n",
        "  core_indices = get_core_indices(D, n, ε, min_points)\n",
        "  core_indices_set = set(core_indices)\n",
        "\n",
        "  # build connected components:\n",
        "  cluster_counter = -1\n",
        "  for i in range(len(core_indices)):\n",
        "    if cluster_id[core_indices[i]] != -1: continue\n",
        "    cluster_counter = cluster_counter + 1\n",
        "    cluster_id[core_indices[i]] = cluster_counter\n",
        "    connect_to_adj_cores(D, n, ε, core_indices[i], cluster_counter, core_indices_set, cluster_id)\n",
        "\n",
        "  return cluster_id"
      ],
      "metadata": {
        "id": "oMUcuNws2vlg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Evaluation"
      ],
      "metadata": {
        "id": "pwAX5BIaD5z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIEiq5-5fc1",
        "outputId": "f73cc053-6320-4764-d144-be83e0d9edfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tPrecision :  0.9999999999999997\n",
            "\tRecall :  0.3181818181818182\n",
            "\tF-score :  0.46190476190476193\n",
            "\tEntropy :  0.0\n"
          ]
        }
      ],
      "source": [
        "# our main code test for mean dataset\n",
        "train_mean_dataset, train_mean_truth, test_dataset, test_truth = get_mean_dataset(naive_data, naive_truth, 0.8)\n",
        "labels = DBSCAN_algorithm(train_mean_dataset, ε = 0.9, min_points = 48)\n",
        "labels = np.array(labels)\n",
        "evaluate(labels, train_mean_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Evaluation"
      ],
      "metadata": {
        "id": "EpahNwzsso5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean"
      ],
      "metadata": {
        "id": "5sWz9dNmzuLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our main code test for mean dataset\n",
        "train_mean_dataset, train_mean_truth, test_dataset, test_truth = get_mean_dataset(naive_data, naive_truth, 0.8)\n",
        "test_mean_dataset, test_mean_truth = dataset_after_mean_reformulation(test_dataset, test_truth)\n",
        "labels_mean_predictions = DBSCAN_algorithm(test_mean_dataset, ε = 2, min_points = 16)\n",
        "labels_mean_predictions = np.array(labels_mean_predictions)\n",
        "evaluate(labels_mean_predictions, train_mean_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOPJuiS6oVwl",
        "outputId": "12c085a8-8951-4390-f678-b8d1373bc3b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tPrecision :  0.7338709677419355\n",
            "\tRecall :  0.25505216095380034\n",
            "\tF-score :  0.3925399282554809\n",
            "\tEntropy :  0.8098210808911355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "cN_9rVDrz0UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the PCA dataset\n",
        "pca, train_pca_dataset, train_pca_truth, test_pca_dataset, test_pca_truth = get_reduced_dataset(naive_data, naive_truth, 0.8)\n",
        "\n",
        "# Reformulate the PCA dataset\n",
        "collapsed_arr = test_pca_dataset.reshape((19 * 8 * 12, 125 * 45))\n",
        "test_pca_truth = test_pca_truth.reshape((19 * 8 * 12))\n",
        "test_pca_dataset = pca.transform(collapsed_arr)"
      ],
      "metadata": {
        "id": "kE4FzUOw2ofi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perfrom normalized cut on test PCA dataset\n",
        "test_pca_predictions = DBSCAN_algorithm(test_pca_dataset, ε = 1, min_points = 10)\n",
        "evaluate(test_pca_predictions, test_pca_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjDFyi6Uz219",
        "outputId": "b28e4d65-9c8e-47de-ba25-33680b615588"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tPrecision :  0.9999999999999998\n",
            "\tRecall :  0.19999999999999998\n",
            "\tF-score :  0.3438625582469251\n",
            "\tEntropy :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perfrom normalized cut on test PCA dataset\n",
        "test_pca_predictions = DBSCAN_algorithm(test_pca_dataset, ε = 1, min_points = 10)\n",
        "print(np.unique(test_pca_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz5HPu5I1-3I",
        "outputId": "ffc32bf6-f48d-4290-aa3c-cef88b4372dc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ygqMuuXcKsx3",
        "HYQcuv3KLCTx",
        "bSJhECMNKOQX",
        "iQAB-SBLL5QG",
        "uzG7HGpEMCiP",
        "-_ktqUbUFZwL",
        "SkIBxUexaElo",
        "4xmdm0n7M9Cq",
        "R9Ha7UHXNJkZ",
        "BchZOHHQyayB",
        "g-7O8fBdt1Hh",
        "l1RcBaSYs7JA",
        "phX3JNTgtBKE",
        "ysPHfsjDtGZ1",
        "8UDCATJYtvtr",
        "J3n1Ff3xt5Pu",
        "vltDG0LFt9Iz",
        "6217bNXFuyth",
        "KmXhW7qryS7f",
        "HrLtEE36u4Tl",
        "sVrqNWdVWkiA",
        "ULOT0SAfVxNz",
        "dQzBskuQGIvb",
        "hY39zqWjFn0z",
        "ls0_ndiVGIvy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}